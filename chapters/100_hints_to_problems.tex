%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% APPENDIX: HINTS AND SELECTED SOLUTIONS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Hints and Selected Solutions}
\addcontentsline{toc}{chapter}{Hints and Selected Solutions}

This appendix provides hints and partial solutions for selected exercises.
Full solutions are not given---working through the problems yourself is
essential for developing intuition.

\section*{Chapter 0}

\paragraph{Exercise 0.1.}
The centroid is $(\bar{L}, \bar{W}) = (4.4, 2.3)$. After adding plant F,
the centroid shifts toward (6.0, 3.5); compute the new mean of all six
observations.

\paragraph{Exercise 0.4.}
Euclidean distance is $\sqrt{2^2 + 3^2} = \sqrt{13} \approx 3.6$ mm (if
both traits are in mm). This number has no direct biological interpretation;
it mixes millimetres of length with millimetres of width.

\section*{Chapter 1}

\paragraph{Exercise 1.1.}
(a) $\|(3,4)\| = 5$. (b) $\|(1,1,1)\| = \sqrt{3}$. (c) $\|(2,-2,1)\| = 3$.
(d) $\|(1,0,0,0,1)\| = \sqrt{2}$.

\paragraph{Exercise 1.3.}
(c) The dot product of $(1,2)$ and $(-2,1)$ is $1(-2) + 2(1) = 0$. A dot
product of zero means the vectors are orthogonal (perpendicular).

\paragraph{Exercise 1.4.}
(a) $\text{proj}_{(1,0)}(3,4) = (3, 0)$. This is the ``shadow'' of $(3,4)$
on the $x$-axis.

\section*{Chapter 2}

\paragraph{Exercise 2.1.}
$\mat{A}$ stretches vectors by factor 2 in the $x$-direction while leaving
the $y$-component unchanged. The unit circle becomes an ellipse with
semi-axes 2 (horizontal) and 1 (vertical).

\paragraph{Exercise 2.2.}
$\mat{R}$ rotates vectors by $90°$ counterclockwise. Applying $\mat{R}$
twice gives $\mat{R}^2 = -\mat{I}$, which is rotation by $180°$.

\paragraph{Exercise 2.4.}
$\mat{A}\mat{B} = \begin{pmatrix} 0 & 2 \\ 3 & 0 \end{pmatrix}$ and
$\mat{B}\mat{A} = \begin{pmatrix} 0 & 3 \\ 2 & 0 \end{pmatrix}$. They
are not equal: matrix multiplication is not commutative.

\section*{Chapter 10}

\paragraph{Exercise 10.2.}
Mean = 6. Deviations: $-4, -2, 0, 2, 4$. Squared deviations: $16, 4, 0, 4, 16$.
Variance = $(16+4+0+4+16)/5 = 8$ (population variance) or $40/4 = 10$
(sample variance with $n-1$).

\paragraph{Exercise 10.4.}
Sum of squared distances: at $c=2$: 12; at $c=3$: 8; at $c=4$: 12. The
minimum is at the mean, $c=3$.

\paragraph{Exercise 10.5.}
In cm: $d = \sqrt{(12-10)^2 + (2.5-2)^2} = \sqrt{4.25} \approx 2.06$. In
mm for wing: $d = \sqrt{(120-100)^2 + (2.5-2)^2} = \sqrt{400.25} \approx 20.0$.
The wing difference now dominates because it's measured in larger numbers.

\section*{Chapter 11}

\paragraph{Exercise 11.1.}
With height in metres: $d(A,B) \approx 2.00$, $d(A,C) \approx 20.0$. With
height in cm: $d(A,B) \approx 5.4$, $d(A,C) \approx 22.4$. Converting to
cm increases the height contribution, but since weight differences dominate
anyway, $d(A,C)$ changes less proportionally.

\paragraph{Exercise 11.4.}
Euclidean distance is appropriate when: (1) all traits are measured in
the same units with similar scales, and (2) traits are uncorrelated. Example
where it works: measurements all in mm on a single structure. Example where
it fails: mass (kg) and length (mm) of animals.

\section*{Chapter 12}

\paragraph{Exercise 12.1.}
$\bar{X} = 4$, $\bar{Y} = 6$. $\Var(X) = 2.5$, $\Var(Y) = 2.5$.
$\Cov(X,Y) = 2.5$. So
$\mat{S} = \begin{pmatrix} 2.5 & 2.5 \\ 2.5 & 2.5 \end{pmatrix}$.
(Note: this matrix is singular---the traits are perfectly correlated.)

\paragraph{Exercise 12.3.}
$\mat{S}^{-1} = \frac{1}{16}\begin{pmatrix} 5 & -2 \\ -2 & 4 \end{pmatrix}$.
$D^2 = (2, 1) \mat{S}^{-1} (2, 1)^\top = \frac{1}{16}(20 - 8 - 4 + 4) = 0.75$.
$D = \sqrt{0.75} \approx 0.87$. Euclidean distance is $\sqrt{5} \approx 2.24$.

\paragraph{Exercise 12.5.}
For a bivariate normal, about 39\% of observations lie within the $D=1$
ellipse (not 68\%). The univariate ``68\% within 1 SD'' rule does not
generalise directly because probability spreads across two dimensions.

\section*{Chapter 32}

\paragraph{Exercise 32.1.}
Eigenvalues: $\lambda_1 = 8$, $\lambda_2 = 2$. Eigenvectors:
$\vect{v}_1 = (1, 1)/\sqrt{2}$, $\vect{v}_2 = (1, -1)/\sqrt{2}$.
PC1 explains $8/10 = 80\%$ of variance. PC1 is ``both traits together''
(size); PC2 is ``traits in opposition'' (shape/contrast).

\paragraph{Exercise 32.3.}
Covariance PCA: PC1 will be dominated by Trait A (variance 100). After
standardising, all traits contribute more equally. Use covariance PCA when
traits are in the same units and you want variance differences to matter.
Use correlation PCA when traits are on different scales or you want to
weight them equally.

\paragraph{Exercise 32.6.}
Total variance = 10. PC1 explains 52\%, PC2 explains 21\%, PC3 explains 10\%.
Cumulative: 52\%, 73\%, 83\%. A common rule is to retain PCs until cumulative
variance exceeds 80\%, suggesting 3 PCs. The ``elbow'' in the scree plot
also suggests 3.